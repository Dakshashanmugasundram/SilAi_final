# ğŸ“š Retrieval-Augmented Generation (RAG) Pipeline â€“ Internship Project

### **Software Developer Intern (AI Team), Ailaysa â€“ Taramani**

This project was developed during my internship at **Ailaysa**, an AI-driven language translation & NLP startup.
My role in the **AI Team** was focused on building a fully functional **RAG (Retrieval-Augmented Generation) pipeline** that converts PDF files into a searchable vector database and enables accurate, context-grounded LLM responses.

This repository contains the core RAG pipeline components that I implemented.

---

## ğŸš€ **Project Overview**

The goal of this work was to create a reliable system that allows an AI model to answer user questions **strictly based on uploaded documents**, reducing hallucinations and improving factual accuracy.

The RAG pipeline performs the following:

1. **PDF Document Loading & Chunking**
2. **Sentence Transformer Embeddings Generation**
3. **FAISS Vector Store Creation (document indexing)**
4. **Context Retrieval for Questions (Top-K Matching)**
5. **LLM Response Generation using Ollama (Qwen2.5)**
6. **Cited, Context-Aware Answers**

---

## ğŸ§© **Architecture**

```
PDF â†’ PyPDFLoader â†’ Text Chunks
        â†“
Embeddings (HuggingFace - mpnet-base-v2)
        â†“
FAISS Vector Store (.pkl)
        â†“
User Query â†’ Retriever â†’ Context
        â†“
LLM Prompting (Ollama - Qwen2.5)
        â†“
Final Answer + Citations
```

---

## ğŸ”§ **Technologies Used**

### **Core RAG Components**

* **LangChain**
* **FAISS Vector Store**
* **HuggingFace Embeddings (all-mpnet-base-v2)**
* **PyPDFLoader** (PDF parsing)
* **Qwen2.5 via Ollama** (LLM inference)

### **Languages**

* Python 3.x

---

## ğŸ“‚ Folder Structure

```
ğŸ“¦ rag-pipeline/
 â”£ ğŸ“œ rag_pipeline.py
 â”£ ğŸ“œ requirements.txt
 â”£ ğŸ“œ README.md
 â”— ğŸ“‚ example_inputs/
       â”— sample.pdf
```

---

## ğŸ§  **Pipeline Steps (What I Built)**

### âœ” 1. **PDF Loading & Text Extraction**

```python
loader = PyPDFLoader(pdf_path)
documents = loader.load()
```

### âœ” 2. **Convert Text â†’ Vector Embeddings**

```python
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    encode_kwargs={"normalize_embeddings": True}
)
```

### âœ” 3. **Create FAISS Vector Store**

```python
db = FAISS.from_documents(documents, embeddings)
```

### âœ” 4. **Save Index as Pickle File**

```python
with open("leader_data.pkl", "wb") as f:
    pickle.dump(db, f)
```

### âœ” 5. **RAG Query â†’ Retrieval + LLM Generation**

```python
retriever = db.as_retriever(search_kwargs={"k": 3})
context_docs = retriever.get_relevant_documents(user_query)
```

### âœ” 6. **Ollama (Qwen2.5) Response Generation**

```python
llm = Ollama(model="qwen2.5", base_url="http://localhost:11434")
response = llm.invoke(prompt_with_context)
```

---

## ğŸ§ª Example Prompt Used in the RAG Pipeline

```txt
You are a helpful assistant. Answer based ONLY on the provided context.

RULES:
- Do NOT hallucinate.
- If the answer can't be found, say "The context does not contain this information."
- Keep the answer clear and factual.

Context:
{context}

User Question:
{input}

Answer:
```

---

## ğŸŒŸ **My Contribution (Internship Work)**

During my internship, I implemented:

### ğŸ”¹ **Complete RAG Pipeline**

* PDF loading
* Document chunking
* Text embedding (MPNet model)
* FAISS vector store creation
* Query-based context retrieval

### ğŸ”¹ **LLM Integration**

* Connected **Ollama (Qwen2.5)** for inference           https://www.prismetric.com/qwen-2-5-what-it-is-and-how-to-use-it/
* Designed prompts to prevent hallucination and ensure accuracy

### ğŸ”¹ **Document-grounded Conversation Logic**

* Strict context enforcement
* Citations extracted from FAISS metadata
* Fallback messages (e.g., unclear query, short input)

Overall, I built a reliable RAG system used by the AI team internally for experimentation and prototype development.

---

## ğŸ“¥ Installation

### 1. Install Dependencies

```
pip install -r requirements.txt
```

### 2. Start Ollama Server

```
ollama run qwen2.5
```

### 3. Run the RAG Pipeline

```
python rag_pipeline.py
```

---

## ğŸ“Œ Future Enhancements

* Support for multilingual embeddings
* Use of persistent vector DBs like Qdrant or Chroma
* Chunking optimization for large PDFs
* Response ranking (relevance scoring)

---

## ğŸ¤ Acknowledgements

Special thanks to the **AI Team HR Team CEO at Ailaysa, Taramani**, for guidance, review, and collaboration during the project.

---
